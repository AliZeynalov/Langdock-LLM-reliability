# LLM Reliability Orchestrator - MVP Specification

**What:** A Go service that makes LLM API calls reliable  
**Demo Time:** 10 minutes, 5 scenarios  
**Deadline:** Thursday mid-day

---

## The Problem We're Solving

When your app calls an LLM provider (OpenAI, Anthropic), things go wrong:
- Timeouts (provider is slow)
- Rate limits (429 errors)
- Server errors (5xx)
- Invalid requests waste money

**Without this system:** Your app crashes or hangs.  
**With this system:** Automatic retry, failover, and protection. App never knows there was a problem.

---

## What You'll Have

### Two Services Running

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   Your App      │────▶│   Orchestrator  │────▶│  Mock Provider  │
│   (curl)        │     │   (port 8080)   │     │  (port 8001)    │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                              │
                        Handles:
                        • Validation
                        • Retries
                        • Failover
                        • Circuit breaker
```

**Orchestrator** (port 8080): Receives requests, handles all reliability logic  
**Mock Provider** (port 8001): Simulates OpenAI/Anthropic with configurable failures

---

## The 5 Demo Scenarios

| # | Scenario | What Happens | Expected Behavior |
|---|----------|--------------|-------------------|
| 1 | **Happy Path** | Normal request | Succeeds on first try (~100ms) |
| 2 | **Timeout + Retry** | Provider is slow | Retries with backoff: 2s → 4s → success |
| 3 | **Rate Limit + Failover** | OpenAI returns 429 | Instantly switches to Anthropic |
| 4 | **Circuit Breaker** | 5+ failures | Skips broken provider automatically |
| 5 | **Validation** | Invalid request | Fails in <10ms, no API call made |

---

## API Contract

### Request (what you send)
```json
POST /v1/chat/completions

{
  "model": "gpt-4",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "temperature": 0.7
}
```

### Response (what you get back)
```json
{
  "request_id": "req_abc123",
  "content": "Hello! How can I help?",
  "provider": "openai",
  "attempts": 1,
  "total_latency_ms": 150
}
```

### Key Response Fields
- `request_id` - Same ID tracks all retries (for debugging)
- `provider` - Which provider actually answered
- `attempts` - How many tries it took
- `total_latency_ms` - End-to-end time

---

## What the Logs Show

During demo, logs clearly show the journey:

```
INFO  request_id=req_abc123 action=received model=gpt-4
INFO  request_id=req_abc123 action=attempt attempt=1 provider=openai
WARN  request_id=req_abc123 action=failed attempt=1 error=timeout
INFO  request_id=req_abc123 action=backoff wait=2s
INFO  request_id=req_abc123 action=attempt attempt=2 provider=openai
INFO  request_id=req_abc123 action=success attempt=2 latency=150ms
```

---

## What's NOT in MVP

To ship on time, we skip:
- ❌ Real API keys (mock provider only)
- ❌ Streaming responses
- ❌ Authentication
- ❌ Database/persistence
- ❌ Half-open circuit breaker state
- ❌ Advanced token counting
- ❌ Metrics dashboard

**These are all valid additions** - but not needed to demonstrate the core patterns.

---

## Success Looks Like

By Thursday mid-day, you can:

1. **Start both services** with `go run`
2. **Run all 5 scenarios** using curl commands
3. **Watch logs** clearly show retry → failover → circuit breaker
4. **Explain the architecture** as you demo
5. **Answer questions** about design decisions

---

## Files We'll Create

```
cmd/
  server/main.go          # Orchestrator entry point
  mock-provider/main.go   # Fake LLM provider

internal/
  models/request.go       # ✅ Already done
  validator/validator.go  # Input validation
  client/client.go        # HTTP client
  client/circuit_breaker.go
  router/router.go        # Provider selection
  orchestrator/orchestrator.go  # Core retry logic
  api/handlers.go         # HTTP endpoints
```

**~8 files total**, each with a single responsibility.

---

## The "Aha" Moment

When you demo Scenario 3 (Rate Limit + Failover):

1. Send request
2. Logs show: "Attempt 1 → OpenAI → 429 Rate Limit"
3. Logs show: "Failover → Anthropic"
4. Logs show: "Attempt 2 → Anthropic → Success"
5. Response returns normally

**The interviewer realizes:** Your app's users never knew OpenAI was overloaded. The system handled it transparently.

---

## Questions This MVP Answers

✅ "How do you handle provider timeouts?" → Retry with exponential backoff  
✅ "What if a provider is completely down?" → Circuit breaker + failover  
✅ "How do you prevent cascading failures?" → Circuit breaker opens after 5 failures  
✅ "How do you debug issues?" → Request ID tracks entire journey  
✅ "How do you save costs?" → Validation fails fast before API call

