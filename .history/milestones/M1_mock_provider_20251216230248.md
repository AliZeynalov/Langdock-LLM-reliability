# M1: Mock Provider

**Goal:** Build a fake LLM provider that simulates real provider behavior with configurable failures.

**Time:** 3-4 hours  
**Output:** HTTP server on :8001 that responds to chat completion requests

---

## Why Mock Provider First?

- Foundation for testing everything else
- No API keys needed
- Controllable failures for demo scenarios
- Can test gateway without external dependencies

---

## API Endpoint

```
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "gpt-4",
  "messages": [{"role": "user", "content": "Hello"}],
  "stream": false
}
```

---

## Query Parameters for Failure Simulation

| Parameter | Example | Behavior |
|-----------|---------|----------|
| `delay` | `?delay=3000` | Adds latency in milliseconds before responding |
| `fail` | `?fail=429` | Returns specified HTTP error code |
| `fail` | `?fail=500` | Returns 500 Internal Server Error |
| `fail` | `?fail=timeout` | Waits 30 seconds (simulates timeout) |
| `stream` | `?stream=true` | Returns SSE streaming response |
| `fail_chunk` | `?fail_chunk=3` | In streaming mode, fails on chunk N |

---

## Response Format

### Non-Streaming Response

```json
{
  "id": "mock-12345",
  "object": "chat.completion",
  "created": 1699000000,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm a mock LLM response. How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 15,
    "total_tokens": 25
  }
}
```

### Streaming Response (SSE)

```
data: {"id":"mock-1","choices":[{"delta":{"content":"Hello"}}]}

data: {"id":"mock-2","choices":[{"delta":{"content":" world"}}]}

data: {"id":"mock-3","choices":[{"delta":{"content":"!"}}]}

data: [DONE]
```

### Error Responses

```json
// 429 Rate Limit
{
  "error": {
    "message": "Rate limit exceeded",
    "type": "rate_limit_error",
    "code": "rate_limit_exceeded"
  }
}

// 500 Server Error
{
  "error": {
    "message": "Internal server error",
    "type": "server_error",
    "code": "internal_error"
  }
}
```

---

## File Structure

```
cmd/mock-provider/
    main.go         # Entry point, HTTP server setup
```

For M1, we keep it simple - one file. We can refactor later if needed.

---

## Implementation Steps

### Step 1: Basic HTTP Server with Gin

```go
package main

import (
    "github.com/gin-gonic/gin"
)

func main() {
    r := gin.Default()
    r.POST("/v1/chat/completions", handleChatCompletion)
    r.Run(":8001")
}
```

### Step 2: Parse Query Parameters

```go
func handleChatCompletion(c *gin.Context) {
    // Get failure simulation params
    delay := c.Query("delay")       // e.g., "3000"
    fail := c.Query("fail")         // e.g., "429", "500", "timeout"
    stream := c.Query("stream")     // e.g., "true"
    failChunk := c.Query("fail_chunk") // e.g., "3"
    
    // Apply delay if specified
    if delay != "" {
        ms, _ := strconv.Atoi(delay)
        time.Sleep(time.Duration(ms) * time.Millisecond)
    }
    
    // Simulate failures
    if fail != "" {
        handleFailure(c, fail)
        return
    }
    
    // Normal response
    if stream == "true" {
        handleStreaming(c, failChunk)
    } else {
        handleNormalResponse(c)
    }
}
```

### Step 3: Implement Failure Modes

```go
func handleFailure(c *gin.Context, failType string) {
    switch failType {
    case "429":
        c.JSON(429, gin.H{
            "error": gin.H{
                "message": "Rate limit exceeded",
                "type":    "rate_limit_error",
                "code":    "rate_limit_exceeded",
            },
        })
    case "500":
        c.JSON(500, gin.H{
            "error": gin.H{
                "message": "Internal server error",
                "type":    "server_error",
                "code":    "internal_error",
            },
        })
    case "timeout":
        // Sleep for 30 seconds to simulate timeout
        time.Sleep(30 * time.Second)
        c.JSON(504, gin.H{
            "error": gin.H{
                "message": "Gateway timeout",
                "type":    "timeout_error",
            },
        })
    }
}
```

### Step 4: Implement Normal Response

```go
func handleNormalResponse(c *gin.Context) {
    response := map[string]interface{}{
        "id":      "mock-" + generateID(),
        "object":  "chat.completion",
        "created": time.Now().Unix(),
        "model":   "gpt-4",
        "choices": []map[string]interface{}{
            {
                "index": 0,
                "message": map[string]interface{}{
                    "role":    "assistant",
                    "content": "Hello! I'm a mock LLM response. How can I help you today?",
                },
                "finish_reason": "stop",
            },
        },
        "usage": map[string]int{
            "prompt_tokens":     10,
            "completion_tokens": 15,
            "total_tokens":      25,
        },
    }
    c.JSON(200, response)
}
```

### Step 5: Implement Streaming Response

```go
func handleStreaming(c *gin.Context, failChunk string) {
    c.Header("Content-Type", "text/event-stream")
    c.Header("Cache-Control", "no-cache")
    c.Header("Connection", "keep-alive")
    
    chunks := []string{"Hello", " from", " streaming", " mock", " provider", "!"}
    failAt := -1
    if failChunk != "" {
        failAt, _ = strconv.Atoi(failChunk)
    }
    
    c.Stream(func(w io.Writer) bool {
        for i, chunk := range chunks {
            // Simulate failure at specific chunk
            if failAt > 0 && i+1 == failAt {
                // Send malformed JSON to simulate failure
                fmt.Fprintf(w, "data: {\"broken\": \n\n")
                return false
            }
            
            data := fmt.Sprintf(`{"id":"mock-%d","choices":[{"delta":{"content":"%s"}}]}`, i, chunk)
            fmt.Fprintf(w, "data: %s\n\n", data)
            w.(http.Flusher).Flush()
            time.Sleep(100 * time.Millisecond) // Simulate generation delay
        }
        
        fmt.Fprintf(w, "data: [DONE]\n\n")
        return false
    })
}
```

---

## Testing Commands

### Happy Path
```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
```

### With Delay
```bash
curl -X POST "http://localhost:8001/v1/chat/completions?delay=2000" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
```

### Rate Limit Error
```bash
curl -X POST "http://localhost:8001/v1/chat/completions?fail=429" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
```

### Streaming
```bash
curl -X POST "http://localhost:8001/v1/chat/completions?stream=true" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
```

### Streaming with Failure
```bash
curl -X POST "http://localhost:8001/v1/chat/completions?stream=true&fail_chunk=3" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
```

---

## Definition of Done

- [ ] Server starts on port 8001
- [ ] Returns valid JSON response for normal requests
- [ ] `?delay=N` adds N milliseconds delay
- [ ] `?fail=429` returns rate limit error
- [ ] `?fail=500` returns server error
- [ ] `?stream=true` returns SSE chunks
- [ ] `?fail_chunk=N` fails on Nth chunk during streaming
- [ ] All responses match OpenAI API format

