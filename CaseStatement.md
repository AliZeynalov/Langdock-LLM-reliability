As Langdock integrates with multiple third-party LLM providers to serve customer requests, we're frequently experiencing reliability issues across the request lifecycle that significantly impact the user experience.
Provider-side failures: Our requests to LLM providers fail in various ways outside our direct control - timeouts, 5xx errors, rate limits enforced unpredictably, and performance degradation under high provider load. For streaming responses, we encounter additional failure modes: streams may stall without indication, disconnect mid-stream leaving partial responses, or deliver malformed chunks (like broken JSON) that we only detect partway through.
Request validation issues: Some requests violate schema requirements, have missing fields, or exceed token limits and fail immediately. However, certain failures only surface mid-generation - for example, when using structured output modes like JSON formatting or tool calls, the provider may accept the request but fail partway through generation after we've already consumed tokens and latency.
Operational visibility gaps: We lack comprehensive metrics on failure modes, latency distributions, retry behavior, and provider health across our integration layer. When retries occur, we can't easily trace a logical request across multiple physical attempts or provider failovers. This makes incident response reactive rather than proactive, with root cause analysis becoming time-consuming and incomplete.